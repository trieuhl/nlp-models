Pre-trained models

1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2019)](https://arxiv.org/pdf/1810.04805.pdf) [[github]](https://github.com/huggingface/transformers)
<p align="center">
    <img src="figs/bert.png" width="500"/>
    <br>
        <em>BERT (Devlin et al., 2019)</em>
</p>
- Compare with:
    - OpenAI GPT: finetune-based, Transformer-based, left-to-right
    - ELMo: feature-based, left-to-right and right-to-left representations
    - GloVe: word representations

- Architecture: Transformer (Vaswani et al., 2017)