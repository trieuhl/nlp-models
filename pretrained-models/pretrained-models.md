Pre-trained models

1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2019)[[paper]](https://arxiv.org/pdf/1810.04805.pdf) [[github]](https://github.com/huggingface/transformers)
<p align="center">
    <img src="figs/bert.png" width="500"/>
    <em>BERT (Devlin et al., 2019)</em>
</p>
